{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-09-28T17:37:25.160869400Z",
     "start_time": "2023-09-28T17:37:24.433829700Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "\n",
    "#check if a A.csv, B.csv, C.csv, D.csv und E.csv datei exisieren wenn ja dann lass sie wenn nein erstell sie neu\n",
    "\n",
    "\n",
    "with open(\"D.csv\", \"w\") as file:\n",
    "    file.write(\"D1,D2,D3,D4,D5,D6,D7,D8,D9,D10,D11,D12A,D12B,D12C,D13,D14A,D15\\n\")\n",
    "\n",
    "with open(\"E.csv\", \"w\") as file:\n",
    "    file.write(\"E1,E3,E4,E5,E6,E7,E8,E9,E10,E11,E12A,E12B,E12C,E13,E14,E15\\n\")\n",
    "\n",
    "dataframe_a = pandas.read_csv('A.csv', sep=',', header=0)\n",
    "dataframe_b = pandas.read_csv('B.csv', sep=',', header=0)\n",
    "dataframe_c = pandas.read_csv('C.csv', sep=',', header=0)\n",
    "dataframe_d = pandas.read_csv('D.csv', sep=',', header=0)\n",
    "dataframe_e = pandas.read_csv('E.csv', sep=',', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'B14'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3802\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[1;34m(self, key, method, tolerance)\u001B[0m\n\u001B[0;32m   3801\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 3802\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine\u001B[38;5;241m.\u001B[39mget_loc(casted_key)\n\u001B[0;32m   3803\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\index.pyx:165\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mKeyError\u001B[0m: 'B14'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[13], line 26\u001B[0m\n\u001B[0;32m     23\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m10\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m     25\u001B[0m \u001B[38;5;66;03m# Anwenden der Funktion auf den DataFrame\u001B[39;00m\n\u001B[1;32m---> 26\u001B[0m dataframe_c[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mC27\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m dataframe_c\u001B[38;5;241m.\u001B[39mapply(set_C27_value, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m     29\u001B[0m \u001B[38;5;66;03m# drop all values from dataframe_d\u001B[39;00m\n\u001B[0;32m     30\u001B[0m \u001B[38;5;66;03m#dataframe_d = dataframe_d.iloc[0:0]\u001B[39;00m\n\u001B[0;32m     31\u001B[0m \u001B[38;5;66;03m# kundennummer bei B2 und C2A verbinde dort nicht alle C2A sind in B2 enthalten\u001B[39;00m\n\u001B[0;32m     32\u001B[0m dataframe_b[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mB2\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m dataframe_b[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mB2\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;28mstr\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:9568\u001B[0m, in \u001B[0;36mDataFrame.apply\u001B[1;34m(self, func, axis, raw, result_type, args, **kwargs)\u001B[0m\n\u001B[0;32m   9557\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mapply\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m frame_apply\n\u001B[0;32m   9559\u001B[0m op \u001B[38;5;241m=\u001B[39m frame_apply(\n\u001B[0;32m   9560\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   9561\u001B[0m     func\u001B[38;5;241m=\u001B[39mfunc,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   9566\u001B[0m     kwargs\u001B[38;5;241m=\u001B[39mkwargs,\n\u001B[0;32m   9567\u001B[0m )\n\u001B[1;32m-> 9568\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m op\u001B[38;5;241m.\u001B[39mapply()\u001B[38;5;241m.\u001B[39m__finalize__(\u001B[38;5;28mself\u001B[39m, method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mapply\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:764\u001B[0m, in \u001B[0;36mFrameApply.apply\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    761\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mraw:\n\u001B[0;32m    762\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapply_raw()\n\u001B[1;32m--> 764\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapply_standard()\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:891\u001B[0m, in \u001B[0;36mFrameApply.apply_standard\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    890\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapply_standard\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m--> 891\u001B[0m     results, res_index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapply_series_generator()\n\u001B[0;32m    893\u001B[0m     \u001B[38;5;66;03m# wrap results\u001B[39;00m\n\u001B[0;32m    894\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwrap_results(results, res_index)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:907\u001B[0m, in \u001B[0;36mFrameApply.apply_series_generator\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    904\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m option_context(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmode.chained_assignment\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m    905\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i, v \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(series_gen):\n\u001B[0;32m    906\u001B[0m         \u001B[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001B[39;00m\n\u001B[1;32m--> 907\u001B[0m         results[i] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mf(v)\n\u001B[0;32m    908\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(results[i], ABCSeries):\n\u001B[0;32m    909\u001B[0m             \u001B[38;5;66;03m# If we have a view on v, we need to make a copy because\u001B[39;00m\n\u001B[0;32m    910\u001B[0m             \u001B[38;5;66;03m#  series_generator will swap out the underlying data\u001B[39;00m\n\u001B[0;32m    911\u001B[0m             results[i] \u001B[38;5;241m=\u001B[39m results[i]\u001B[38;5;241m.\u001B[39mcopy(deep\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "Cell \u001B[1;32mIn[13], line 3\u001B[0m, in \u001B[0;36mset_C27_value\u001B[1;34m(row)\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mset_C27_value\u001B[39m(row):\n\u001B[0;32m      2\u001B[0m     \u001B[38;5;66;03m# Wert: 90 - Ohne Ausschluss\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mall\u001B[39m(row[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mB14\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mN\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mall\u001B[39m(row[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mC20\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mN\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[0;32m      4\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m90\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m      6\u001B[0m     \u001B[38;5;66;03m# Wert: 01 - Ausschluss nur nach EinSiG\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:981\u001B[0m, in \u001B[0;36mSeries.__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m    978\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_values[key]\n\u001B[0;32m    980\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m key_is_scalar:\n\u001B[1;32m--> 981\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_value(key)\n\u001B[0;32m    983\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_hashable(key):\n\u001B[0;32m    984\u001B[0m     \u001B[38;5;66;03m# Otherwise index.get_value will raise InvalidIndexError\u001B[39;00m\n\u001B[0;32m    985\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    986\u001B[0m         \u001B[38;5;66;03m# For labels that don't resolve as scalars like tuples and frozensets\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:1089\u001B[0m, in \u001B[0;36mSeries._get_value\u001B[1;34m(self, label, takeable)\u001B[0m\n\u001B[0;32m   1086\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_values[label]\n\u001B[0;32m   1088\u001B[0m \u001B[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001B[39;00m\n\u001B[1;32m-> 1089\u001B[0m loc \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindex\u001B[38;5;241m.\u001B[39mget_loc(label)\n\u001B[0;32m   1090\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindex\u001B[38;5;241m.\u001B[39m_get_values_for_loc(\u001B[38;5;28mself\u001B[39m, loc, label)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3804\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[1;34m(self, key, method, tolerance)\u001B[0m\n\u001B[0;32m   3802\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine\u001B[38;5;241m.\u001B[39mget_loc(casted_key)\n\u001B[0;32m   3803\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[1;32m-> 3804\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n\u001B[0;32m   3805\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[0;32m   3806\u001B[0m     \u001B[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001B[39;00m\n\u001B[0;32m   3807\u001B[0m     \u001B[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001B[39;00m\n\u001B[0;32m   3808\u001B[0m     \u001B[38;5;66;03m#  the TypeError.\u001B[39;00m\n\u001B[0;32m   3809\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_indexing_error(key)\n",
      "\u001B[1;31mKeyError\u001B[0m: 'B14'"
     ]
    }
   ],
   "source": [
    "def set_C27_value(row):\n",
    "    # Wert: 90 - Ohne Ausschluss\n",
    "    \n",
    "    #finde die reihe aus B wo B2 = C2A\n",
    "    #wenn B14 = N und C20 = N dann 90\n",
    "    \n",
    "    b_row_14 = dataframe_b.loc[dataframe_b['B2'] == row['C2A'], \"B14\"]\n",
    "    \n",
    "    if all(b_row_14 == 'N') and all(row['C20'] == 'N'):\n",
    "        return '90'\n",
    "\n",
    "    # Wert: 01 - Ausschluss nur nach EinSiG\n",
    "    if any(b_row_14[0:15] == 'Y') and all(b_row_14[15:50] == 'N') and all(row['C20'][10:50] == 'N'):\n",
    "        return '01'\n",
    "    elif any(row['C20'][0:10] == 'Y') and all(row['C20'][10:50] == 'N') and all(b_row_14[15:50] == 'N'):\n",
    "        return '01'\n",
    "\n",
    "    # Wert: 20 - Ausschluss nur nach ESF-Statut\n",
    "    if any(b_row_14[30:50] == 'Y') and all(b_row_14[0:30] == 'N') and all(row['C20'][0:30] == 'N'):\n",
    "        return '20'\n",
    "    elif any(row['C20'][30:50] == 'Y') and all(row['C20'][0:30] == 'N') and all(b_row_14[0:30] == 'N'):\n",
    "        return '20'\n",
    "\n",
    "    # Wert: 11 - Ausschluss \"Bagatellgrenze\" nach EinSiG und ESF-Statut\n",
    "    if all(row['C20'][1:49] == 'N') and row['C20'][49] == 'Y':\n",
    "        return '11'\n",
    "\n",
    "    # Wert: 10 - Ausschluss nach EinSiG und ESF-Statut (alle anderen Fälle)\n",
    "    return '10'\n",
    "\n",
    "\n",
    "# Anwenden der Funktion auf den DataFrame\n",
    "dataframe_c['C27'] = dataframe_c.apply(set_C27_value, axis=1)\n",
    "\n",
    "# drop all values from dataframe_d\n",
    "#dataframe_d = dataframe_d.iloc[0:0]\n",
    "# kundennummer bei B2 und C2A verbinde dort nicht alle C2A sind in B2 enthalten\n",
    "dataframe_b['B2'] = dataframe_b['B2'].astype(str)\n",
    "dataframe_c['C2A'] = dataframe_c['C2A'].astype(str)\n",
    "dataframe_d['D1'] = 'D'\n",
    "dataframe_b[\"B1\"] = \"B\"\n",
    "dataframe_c[\"C1\"] = \"C\"\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-26T17:29:57.064642500Z",
     "start_time": "2023-09-26T17:29:56.994173300Z"
    }
   },
   "id": "76d39d3e3833284"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pandas' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 17\u001B[0m\n\u001B[0;32m     13\u001B[0m     dataframe_a_b_c\u001B[38;5;241m.\u001B[39mto_csv(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mA_B_C.csv\u001B[39m\u001B[38;5;124m'\u001B[39m, index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m     14\u001B[0m     \u001B[38;5;66;03m# save dataframe b to csv\u001B[39;00m\n\u001B[1;32m---> 17\u001B[0m generate_a_b_c_data()\n",
      "Cell \u001B[1;32mIn[1], line 7\u001B[0m, in \u001B[0;36mgenerate_a_b_c_data\u001B[1;34m()\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mglobal\u001B[39;00m dataframe_b_c\n\u001B[0;32m      6\u001B[0m \u001B[38;5;66;03m# verbinde die den dataframe B und C anhand der werte aus B2 und C2A\u001B[39;00m\n\u001B[1;32m----> 7\u001B[0m dataframe_b_c \u001B[38;5;241m=\u001B[39m pandas\u001B[38;5;241m.\u001B[39mmerge(dataframe_b, dataframe_c, left_on\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mB2\u001B[39m\u001B[38;5;124m'\u001B[39m, right_on\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mC2A\u001B[39m\u001B[38;5;124m'\u001B[39m, how\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124minner\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      8\u001B[0m \u001B[38;5;66;03m# Repliziere dataframe_a für die Anzahl der Zeilen in dataframe_b_c\u001B[39;00m\n\u001B[0;32m      9\u001B[0m df_a_repeated_for_b_c \u001B[38;5;241m=\u001B[39m pandas\u001B[38;5;241m.\u001B[39mconcat([dataframe_a] \u001B[38;5;241m*\u001B[39m dataframe_b_c\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m], ignore_index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'pandas' is not defined"
     ]
    }
   ],
   "source": [
    "def generate_a_b_c_data():\n",
    "    global dataframe_b\n",
    "    global dataframe_c\n",
    "    global dataframe_a_b_c\n",
    "    global dataframe_b_c\n",
    "    # verbinde die den dataframe B und C anhand der werte aus B2 und C2A\n",
    "    dataframe_b_c = pandas.merge(dataframe_b, dataframe_c, left_on='B2', right_on='C2A', how='inner')\n",
    "    # Repliziere dataframe_a für die Anzahl der Zeilen in dataframe_b_c\n",
    "    df_a_repeated_for_b_c = pandas.concat([dataframe_a] * dataframe_b_c.shape[0], ignore_index=True)\n",
    "    # Verbinde df_a_repeated_for_b_c horizontal mit dataframe_b_c\n",
    "    dataframe_a_b_c = pandas.concat([df_a_repeated_for_b_c, dataframe_b_c], axis=1)\n",
    "    dataframe_a_b_c[\"A1\"] = \"A\"\n",
    "    dataframe_a_b_c.to_csv('A_B_C.csv', index=False)\n",
    "    # save dataframe b to csv\n",
    "\n",
    "\n",
    "generate_a_b_c_data()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-28T17:31:51.302148100Z",
     "start_time": "2023-09-28T17:31:50.947213100Z"
    }
   },
   "id": "f8c894611415b573"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "#######D2########\n",
    "# trage in D2 die einzigartigen werte aus B2 ein\n",
    "dataframe_d['D2'] = dataframe_a_b_c['B2'].unique()\n",
    "\n",
    "#######D3#######\n",
    "# D3 hat die summe aller werte aus C19 abhängig von der kundennummer\n",
    "dataframe_d['D3'] = dataframe_a_b_c.groupby('B2')['C19'].transform('sum')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-26T17:29:57.065142600Z",
     "start_time": "2023-09-26T17:29:57.065142600Z"
    }
   },
   "id": "ba5d195dfd49faba"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "#######D4#######\n",
    "def compute_D4_grouped(group, df_d):\n",
    "    # Initialize D4 value\n",
    "    D4_value = 0.00\n",
    "\n",
    "    # Check for \"01\" or \"10\" in A3\n",
    "    if str(group['A3'].iloc[0]) in ['01', '10']:\n",
    "\n",
    "        # Check if any value in B14 (range \"01\" to \"30\") is \"Y\"\n",
    "        if any(\"Y\" in s for s in group['B14'].str[0:30]):\n",
    "            D4_value = group['C19'].sum()\n",
    "        elif int(df_d[df_d['D2'] == group['B2'].iloc[0]]['D3'].iloc[0]) < 20 and all(group['C20'].str[0] == 'Y'):\n",
    "            D4_value = df_d[df_d['D2'] == group['B2'].iloc[0]]['D3'].iloc[0]\n",
    "        elif any(\"Y\" in s for s in group['C20'].str[1:30]):\n",
    "            D4_value = group['C19'].sum()\n",
    "    else:\n",
    "        D4_value = 0.00\n",
    "\n",
    "    # Set D4 value in dataframe_d\n",
    "    df_d.loc[df_d['D2'] == group['B2'].iloc[0], 'D4'] = D4_value\n",
    "\n",
    "\n",
    "# group by B2 and apply the function compute_D4_grouped\n",
    "dataframe_a_b_c.groupby('B2').apply(compute_D4_grouped, dataframe_d)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-26T17:29:57.065641700Z",
     "start_time": "2023-09-26T17:29:57.065641700Z"
    }
   },
   "id": "6d525f3528546a60"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "#######D5#############\n",
    "def compute_D5_grouped(group, df_d):\n",
    "    # Initialize D5 value\n",
    "    D5_value = 0.00\n",
    "\n",
    "    # Check for \"01\" or \"10\" in A3\n",
    "    if str(group['A3'].iloc[0]) in ['01', '10']:\n",
    "        # Calculate D3 divided by D4\n",
    "        D3_value = df_d[df_d['D2'] == group['B2'].iloc[0]]['D3'].iloc[0]\n",
    "        D4_value = df_d[df_d['D2'] == group['B2'].iloc[0]]['D4'].iloc[0]\n",
    "\n",
    "        # Avoid division by zero\n",
    "        if D4_value != 0:\n",
    "            D5_value = D3_value / D4_value\n",
    "    else:\n",
    "        D5_value = 0.00\n",
    "\n",
    "    # Set D5 value in dataframe_d\n",
    "    df_d.loc[df_d['D2'] == group['B2'].iloc[0], 'D5'] = D5_value\n",
    "\n",
    "\n",
    "dataframe_a_b_c.groupby('B2').apply(compute_D5_grouped, dataframe_d)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-26T17:29:57.066141300Z",
     "start_time": "2023-09-26T17:29:57.066141300Z"
    }
   },
   "id": "408ddc0c935ffd8c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "########################## D6 #############################\n",
    "def compute_D6_grouped(group, df_d):\n",
    "    # Initialize D6 value\n",
    "    D6_value = 0.00\n",
    "\n",
    "    # Check for \"01\" or \"10\" in A3\n",
    "    if str(group['A3'].iloc[0]) in ['01', '10']:\n",
    "        # Get the D5 value\n",
    "        D5_value = df_d[df_d['D2'] == group['B2'].iloc[0]]['D5'].iloc[0]\n",
    "\n",
    "        # Calculate the limit which is A4 * C5\n",
    "        limit_value = group['A4'].iloc[0] * group['C5'].iloc[0]\n",
    "\n",
    "        # Set D6 to be the minimum of D5 and the limit value\n",
    "        D6_value = min(D5_value, limit_value)\n",
    "    else:\n",
    "        D6_value = 0.00\n",
    "\n",
    "    # Set D6 value in dataframe_d\n",
    "    df_d.loc[df_d['D2'] == group['B2'].iloc[0], 'D6'] = D6_value\n",
    "\n",
    "\n",
    "dataframe_a_b_c.groupby('B2').apply(compute_D6_grouped, dataframe_d)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-26T17:29:57.067647800Z",
     "start_time": "2023-09-26T17:29:57.067144400Z"
    }
   },
   "id": "f55ff8bb74e5a8ec"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataframe_a_b_c' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 21\u001B[0m\n\u001B[0;32m     17\u001B[0m     \u001B[38;5;66;03m# Set D7 value in dataframe_d\u001B[39;00m\n\u001B[0;32m     18\u001B[0m     df_d\u001B[38;5;241m.\u001B[39mloc[df_d[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mD2\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m==\u001B[39m group[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mB2\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39miloc[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mD7\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m D7_value\n\u001B[1;32m---> 21\u001B[0m dataframe_a_b_c\u001B[38;5;241m.\u001B[39mgroupby(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mB2\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mapply(compute_D7_grouped, dataframe_d)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'dataframe_a_b_c' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "def compute_D7_grouped(group, df_d):\n",
    "    # Initialize D7 value\n",
    "    D7_value = 0.00\n",
    "\n",
    "    # Check for \"01\" or \"10\" in A3\n",
    "    if str(group['A3'].iloc[0]) in ['01', '10']:\n",
    "        # Get the D5 and D6 values\n",
    "        D5_value = df_d[df_d['D2'] == group['B2'].iloc[0]]['D5'].iloc[0]\n",
    "        D6_value = df_d[df_d['D2'] == group['B2'].iloc[0]]['D6'].iloc[0]\n",
    "\n",
    "        # Avoid division by zero\n",
    "        if D6_value != 0:\n",
    "            D7_value = D5_value / D6_value\n",
    "    else:\n",
    "        D7_value = 0.00\n",
    "\n",
    "    # Set D7 value in dataframe_d\n",
    "    df_d.loc[df_d['D2'] == group['B2'].iloc[0], 'D7'] = D7_value\n",
    "\n",
    "\n",
    "dataframe_a_b_c.groupby('B2').apply(compute_D7_grouped, dataframe_d)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-26T17:53:40.747035200Z",
     "start_time": "2023-09-26T17:53:40.399405800Z"
    }
   },
   "id": "f681e7f3da271002"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "########################## D8 #############################\n",
    "def compute_D8_grouped(group, df_d):\n",
    "    # Check if C22 has a value and it's not \"DE\"\n",
    "    D8_value = 0.00\n",
    "    if pandas.notnull(group['C22'].iloc[0]) and group['C22'].iloc[0] != \"DE\":\n",
    "        D8_value = group['C19'].sum()\n",
    "\n",
    "    df_d.loc[df_d['D2'] == group['B2'].iloc[0], 'D8'] = D8_value\n",
    "\n",
    "\n",
    "dataframe_a_b_c.groupby('B2').apply(compute_D8_grouped, dataframe_d)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-26T17:29:57.068647800Z"
    }
   },
   "id": "e2fbb9bc85f368ca"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "########################## D9 #############################\n",
    "def compute_D9_grouped(group, df_d):\n",
    "    D9_value = 0.00\n",
    "    if str(group['A3'].iloc[0]) == '20':\n",
    "        limit_value = group['A5'].iloc[0] * group['C5'].iloc[0]\n",
    "        D9_value = min(df_d[df_d['D2'] == group['B2'].iloc[0]]['D3'].iloc[0], limit_value)\n",
    "    df_d.loc[df_d['D2'] == group['B2'].iloc[0], 'D9'] = D9_value\n",
    "\n",
    "\n",
    "dataframe_a_b_c.groupby('B2').apply(compute_D9_grouped, dataframe_d)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-26T17:29:57.069149900Z"
    }
   },
   "id": "c85bc1656c356ae8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "########################## D10 #############################\n",
    "\n",
    "def compute_D10_grouped(group, df_d):\n",
    "    D10_value = 0.00\n",
    "    if str(group['A3'].iloc[0]) in [\"10\", \"20\"]:\n",
    "        D10_value = group[group['C27'] == 10]['C19'].sum()\n",
    "    df_d.loc[df_d['D2'] == group['B2'].iloc[0], 'D10'] = D10_value\n",
    "\n",
    "\n",
    "dataframe_a_b_c.groupby('B2').apply(compute_D10_grouped, dataframe_d)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-26T17:29:57.069648500Z"
    }
   },
   "id": "c87e5c4c9bc6658"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def compute_D11_grouped(group, df_d):\n",
    "    D11_value = 0.00\n",
    "    if str(group['A3'].iloc[0]) in ['10', '20']:\n",
    "        D11_value = group[group['C27'] == 20]['C19'].sum() - group['C26'].sum()\n",
    "    df_d.loc[df_d['D2'] == group['B2'].iloc[0], 'D11'] = D11_value\n",
    "\n",
    "\n",
    "dataframe_a_b_c.groupby('B2').apply(compute_D11_grouped, dataframe_d)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-26T17:29:57.070148Z"
    }
   },
   "id": "bfc687eb115afb8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "########################## D14A #############################\n",
    "\n",
    "def compute_D14A_grouped(group, df_d):\n",
    "    D14A_value = 0.00\n",
    "    if str(group['A3'].iloc[0]) in [\"10\", \"20\"]:\n",
    "        # Sum of debitorische Kontosalden from C19\n",
    "        D14A_value = -group['C19'].sum()\n",
    "    else:\n",
    "        D14A_value = 0.00\n",
    "\n",
    "    df_d.loc[df_d['D2'] == group['B2'].iloc[0], 'D14A'] = D14A_value\n",
    "\n",
    "\n",
    "dataframe_a_b_c.groupby('B2').apply(compute_D14A_grouped, dataframe_d)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-26T17:29:57.071241700Z"
    }
   },
   "id": "1d537c6d60b44d37"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "########################## D12A #############################\n",
    "def compute_D12A_grouped(group, df_d):\n",
    "    D12A_value = 0.00\n",
    "    if str(group['A3'].iloc[0]) in [\"10\", \"20\"]:\n",
    "\n",
    "        wert1 = df_d.loc[df_d['D2'] == group['B2'].iloc[0], 'D3'].iloc[0]\n",
    "        wert2 = df_d.loc[df_d['D2'] == group['B2'].iloc[0], 'D10'].iloc[0]\n",
    "        wert3 = df_d.loc[df_d['D2'] == group['B2'].iloc[0], 'D14A'].iloc[0]\n",
    "        wert4 = max(df_d.loc[df_d['D2'] == group['B2'].iloc[0], ['D6', 'D9', 'D11']].iloc[0])\n",
    "        HW1 = wert1 - wert2 + wert3 - wert4\n",
    "\n",
    "        wert5 = df_d.loc[df_d['D2'] == group['B2'].iloc[0], 'D6'].iloc[0] + df_d.loc[df_d['D2'] == group['B2'].iloc[0], 'D9'].iloc[0];\n",
    "        if wert5 != 0:\n",
    "            limit_value = (group['A6C'].iloc[0] * group['C5'].iloc[0]) / wert5\n",
    "            D12A_value = min(HW1, limit_value)\n",
    "        else:\n",
    "            D12A_value = HW1\n",
    "        if D12A_value < 0:\n",
    "            D12A_value = 0.00\n",
    "\n",
    "    df_d.loc[df_d['D2'] == group['B2'].iloc[0], 'D12A'] = D12A_value\n",
    "\n",
    "\n",
    "dataframe_a_b_c.groupby('B2').apply(compute_D12A_grouped, dataframe_d)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-26T17:29:57.071745800Z"
    }
   },
   "id": "b96b6bde72652856"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataframe_a_b_c' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 36\u001B[0m\n\u001B[0;32m     31\u001B[0m         D12B_value \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmin\u001B[39m(value1, value2, value3)\n\u001B[0;32m     33\u001B[0m     df_d\u001B[38;5;241m.\u001B[39mloc[df_d[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mD2\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m==\u001B[39m group[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mB2\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39miloc[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mD12B\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m D12B_value\n\u001B[1;32m---> 36\u001B[0m dataframe_a_b_c\u001B[38;5;241m.\u001B[39mgroupby(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mB2\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mapply(compute_D12B_grouped, dataframe_d)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'dataframe_a_b_c' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "########################## D12B #############################\n",
    "def compute_D12B_grouped(group, df_d):\n",
    "    D12B_value = 0.00\n",
    "\n",
    "    if str(group['A3'].iloc[0]) in [10, 20] and pandas.notnull(group['A8'].iloc[0]):\n",
    "        # Berechne HW2\n",
    "        HW2 = group[(group['C27'].isin(['01', '90'])) & (group['C21'].str[18] == 'Y')]['C19'].sum()\n",
    "        wert1 = df_d.loc[df_d['D2'] == group['B2'].iloc[0], 'D3'].iloc[0]\n",
    "        wert2 = df_d.loc[df_d['D2'] == group['B2'].iloc[0], 'D10'].iloc[0]\n",
    "        wert3 = df_d.loc[df_d['D2'] == group['B2'].iloc[0], 'D14A'].iloc[0]\n",
    "        wert4 = max(df_d.loc[df_d['D2'] == group['B2'].iloc[0], ['D6', 'D9', 'D11']].iloc[0])\n",
    "        HW1 = wert1 - wert2 + wert3 - wert4\n",
    "        # Berechne HW3\n",
    "        HW3_1 = group[(group['C27'] == '90') & (group['C21'].str[10] == 'N') & (group['C21'].str[18] == 'N')]['C19'].sum()\n",
    "        HW3_2 = group[group['C27'] == '20']['C19'].sum()\n",
    "        HW3 = HW3_1 + HW3_2\n",
    "\n",
    "        # Werte für die Berechnung\n",
    "        D12A_value = df_d.loc[df_d['D2'] == group['B2'].iloc[0], 'D12A'].iloc[0]\n",
    "        D6_value = df_d.loc[df_d['D2'] == group['B2'].iloc[0], 'D6'].iloc[0]\n",
    "        D9_value = df_d.loc[df_d['D2'] == group['B2'].iloc[0], 'D9'].iloc[0]\n",
    "        A9_value = group['A9'].iloc[0]\n",
    "        C5_value = group['C5'].iloc[0]\n",
    "\n",
    "        # Berechne die drei möglichen Werte für D12B\n",
    "        value1 = HW1 / D12A_value\n",
    "        value2 = HW2 / (D6_value + D9_value / HW3) if HW3 != 0 and (D6_value + D9_value / HW3) > 0 else HW2\n",
    "        value3 = A9_value * C5_value / (D6_value + D9_value + D12A_value)\n",
    "\n",
    "        # Setze D12B auf den kleinsten Wert\n",
    "        D12B_value = min(value1, value2, value3)\n",
    "\n",
    "    df_d.loc[df_d['D2'] == group['B2'].iloc[0], 'D12B'] = D12B_value\n",
    "\n",
    "\n",
    "dataframe_a_b_c.groupby('B2').apply(compute_D12B_grouped, dataframe_d)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-28T15:34:32.559790700Z",
     "start_time": "2023-09-28T15:34:32.162364100Z"
    }
   },
   "id": "8146cb733bb78f0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "########################## D12C #############################\n",
    "def compute_D12C_grouped(group, df_d):\n",
    "    D12C_value = 0.00\n",
    "\n",
    "    # Berechne HW4\n",
    "    HW4 = group[(group['C27'].isin(['01', '90'])) & (group['C21'].str[10] == 'Y')]['C19'].sum()\n",
    "\n",
    "    # Berechne HW5\n",
    "    HW5_1 = group[(group['C27'] == '90') & (group['C21'].str[10] == 'N')]['C19'].sum()\n",
    "    HW5_2 = group[group['C27'] == '20']['C19'].sum()\n",
    "    HW5 = HW5_1 + HW5_2\n",
    "\n",
    "    # Berechne HW1\n",
    "    wert1 = df_d.loc[df_d['D2'] == group['B2'].iloc[0], 'D3'].iloc[0]\n",
    "    wert2 = df_d.loc[df_d['D2'] == group['B2'].iloc[0], 'D10'].iloc[0]\n",
    "    wert3 = df_d.loc[df_d['D2'] == group['B2'].iloc[0], 'D14A'].iloc[0]\n",
    "    wert4 = max(df_d.loc[df_d['D2'] == group['B2'].iloc[0], ['D6', 'D9', 'D11']].iloc[0])\n",
    "    HW1 = wert1 - wert2 + wert3 - wert4\n",
    "\n",
    "    if str(group['A3'].iloc[0]) in [\"10\", \"20\"] and any(group['C21'].str[10] == 'Y') and HW1 < int(group['A6C'].iloc[0]) * group['C5'].sum():  #welcher c5 wert soll genommen werden\n",
    "        # Werte für die Berechnung\n",
    "        D12A_value = df_d.loc[df_d['D2'] == group['B2'].iloc[0], 'D12A'].iloc[0]\n",
    "        D12B_value = df_d.loc[df_d['D2'] == group['B2'].iloc[0], 'D12B'].iloc[0]\n",
    "        D6_value = df_d.loc[df_d['D2'] == group['B2'].iloc[0], 'D6'].iloc[0]\n",
    "        D9_value = df_d.loc[df_d['D2'] == group['B2'].iloc[0], 'D9'].iloc[0]\n",
    "        A6_value = group['A6C'].iloc[0]\n",
    "        C5_value = group['C5'].iloc[0]\n",
    "        A7_value = group['A7'].iloc[0]\n",
    "\n",
    "        wert2 = HW4 / ((D6_value + D9_value) / HW5) if (D6_value + D9_value) / HW5 > 0 else HW4\n",
    "        wert3 = A7_value * C5_value / (D6_value + D9_value + D12A_value + D12B_value)\n",
    "\n",
    "        # Setze D12C auf den kleinsten Wert\n",
    "        D12C_value = min(wert1, wert2, wert3)\n",
    "        if D12C_value < 0:\n",
    "            D12C_value = 0.00\n",
    "\n",
    "    df_d.loc[df_d['D2'] == group['B2'].iloc[0], 'D12C'] = D12C_value\n",
    "\n",
    "\n",
    "dataframe_a_b_c.groupby('B2').apply(compute_D12C_grouped, dataframe_d)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-26T17:29:57.115860300Z",
     "start_time": "2023-09-26T17:29:57.073244800Z"
    }
   },
   "id": "fbe6b482223f70ae"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def compute_D13_grouped(group, df_d):\n",
    "    D13_value = 0.00\n",
    "    if str(group['A3'].iloc[0]) in [\"10\", \"20\"]:\n",
    "        D12A_value = df_d.loc[df_d['D2'] == group['B2'].iloc[0], 'D12A'].iloc[0]\n",
    "        D12B_value = df_d.loc[df_d['D2'] == group['B2'].iloc[0], 'D12B'].iloc[0]\n",
    "        D12C_value = df_d.loc[df_d['D2'] == group['B2'].iloc[0], 'D12C'].iloc[0]\n",
    "        D13_value = D12A_value + D12B_value + D12C_value\n",
    "    df_d.loc[df_d['D2'] == group['B2'].iloc[0], 'D13'] = D13_value\n",
    "\n",
    "\n",
    "dataframe_a_b_c.groupby('B2').apply(compute_D13_grouped, dataframe_d)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-26T17:29:57.074244800Z"
    }
   },
   "id": "d0dff8773c5c2821"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def compute_D15_grouped(group, df_d):\n",
    "    D15_value = 0.00\n",
    "    valid_entries = group[pandas.notnull(group['C23'])]\n",
    "    D15_value = valid_entries['C19'].sum()\n",
    "    df_d.loc[df_d['D2'] == group['B2'].iloc[0], 'D15'] = D15_value\n",
    "\n",
    "\n",
    "dataframe_a_b_c.groupby('B2').apply(compute_D15_grouped, dataframe_d)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-26T17:29:57.074745100Z"
    }
   },
   "id": "90c141229cde391c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "###############################################################\n",
    "dataframe_d.to_csv('D.csv', index=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-26T17:29:57.075244500Z"
    }
   },
   "id": "d166464aee9d6e15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Erstelle ein leeres DataFrame für E\n",
    "\n",
    "dataframe_e = pandas.DataFrame(header=0)\n",
    "\n",
    "# Setze die Werte für die Spalten in E\n",
    "dataframe_e.at[0, 'E1'] = 'E'\n",
    "# E2 wird freigelassen, wie du erwähnt hast\n",
    "dataframe_e.at[0, 'E3'] = dataframe_d['D3'].sum()\n",
    "dataframe_e.at[0, 'E4'] = dataframe_d['D4'].sum()\n",
    "dataframe_e.at[0, 'E5'] = dataframe_d['D5'].sum()\n",
    "dataframe_e.at[0, 'E6'] = dataframe_d['D6'].sum()\n",
    "dataframe_e.at[0, 'E7'] = dataframe_d['D7'].sum()\n",
    "dataframe_e.at[0, 'E8'] = dataframe_d['D8'].sum()\n",
    "dataframe_e.at[0, 'E9'] = dataframe_d['D9'].sum()\n",
    "dataframe_e.at[0, 'E10'] = dataframe_d['D10'].sum()\n",
    "dataframe_e.at[0, 'E11'] = dataframe_d['D11'].sum()\n",
    "dataframe_e.at[0, 'E12A'] = dataframe_d['D12A'].sum()\n",
    "dataframe_e.at[0, 'E12B'] = dataframe_d['D12B'].sum()\n",
    "dataframe_e.at[0, 'E12C'] = dataframe_d['D12C'].sum()\n",
    "dataframe_e.at[0, 'E13'] = dataframe_d['D13'].sum()\n",
    "dataframe_e.at[0, 'E14'] = dataframe_d['D14A'].sum()\n",
    "dataframe_e.at[0, 'E15'] = dataframe_d['D15'].sum()\n",
    "\n",
    "dataframe_e.to_csv('E.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-26T17:29:57.075744800Z"
    }
   },
   "id": "c027299f8b63153c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "#output datei schreiben:\n",
    "with open(\"output.txt\", \"w\") as file:\n",
    "    file.write(\"\\t\".join(dataframe_a.iloc[0].astype(str)) + \"\\n\")\n",
    "    # Gruppieren nach allen B-Spalten\n",
    "    grouped = dataframe_a_b_c.groupby(list(dataframe_a_b_c.columns[dataframe_a_b_c.columns.str.startswith('B')]))\n",
    "    for _, group in grouped:\n",
    "        # Schreibe die B-Werte in die Datei\n",
    "        b_values = group.iloc[0][dataframe_a_b_c.columns.str.startswith('B')].astype(str)\n",
    "        file.write(','.join(b_values) + '\\n')\n",
    "\n",
    "        # Schreibe die C-Werte in die Datei\n",
    "        for _, row in group.iterrows():\n",
    "            c_values = row[dataframe_a_b_c.columns.str.startswith('C')].astype(str)\n",
    "            file.write(','.join(c_values) + '\\n')\n",
    "\n",
    "        # Schreibe die D-Werte in die Datei\n",
    "        b2_value = group['B2'].iloc[0]\n",
    "        d_row = dataframe_d[dataframe_d['D2'] == b2_value]\n",
    "        d_values = d_row.iloc[0].astype(str)\n",
    "        file.write(','.join(d_values) + '\\n')\n",
    "\n",
    "    # Schreibe die E-Werte in die Datei\n",
    "    for _, row in dataframe_e.iterrows():\n",
    "        e_values = row.astype(str)\n",
    "        file.write(','.join(e_values) + '\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-26T17:29:57.076244300Z"
    }
   },
   "id": "c67ad7f691a801dd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "\n",
    "#get current date\n",
    "def analysiere_wert(dataframe_b, row):\n",
    "    #element:\n",
    "    #df_d.loc[df_d['D2'] == group['B2'].iloc[0], 'D12B'] = D12B_value\n",
    "    if dataframe_b.loc(dataframe_b['B2'] == row['C2'], 'B16') >=20:\n",
    "    #get wert aus C25 aus der row: Datarow aus pandas\n",
    "    month = row[\"C25\"] \n",
    "    #prüfe ob der wert in C25 None ist\n",
    "    if month is not None:\n",
    "        return\n",
    "    #Fälligkeit spalte C14 Laufzeit Spalte C25  Kündigungsfrist C26\n",
    "    \n",
    "#C14 -> fälligkeit datum\n",
    "\n",
    "    faelligkeit = datetime.datetime.strptime(row[\"C14\"], \"%d%m%Y\")\n",
    "    \n",
    "    \n",
    "    #C6 -> Kontoeröffnung\n",
    "    #C5 -> Stichtag der ED\n",
    "    #Datumformat: 01012020\n",
    "\n",
    "\n",
    "dataframe_c.apply(analysiere_wert, axis=1, args=(dataframe_b)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cd7b365288e4fc47"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "8ab2bd68c4403d07"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
